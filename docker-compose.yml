services:
  llama:
    build:
      context: images/llama
      dockerfile: Dockerfile
    container_name: llama
    ports:
      - '8000:8080' # Porta externa 8000, interna 8080 no container
    volumes:
      - ./models:/models
    command: ./build/bin/llama-server -m /models/mistral.gguf --n_threads 8 -c 4096 --host 0.0.0.0 --port 8080
    restart: unless-stopped

  # oraculo:
  #   build:
  #     context: .
  #     dockerfile: images/node/Dockerfile
  #   container_name: oraculo
  #   ports:
  #     - '8080:8080'
  #   volumes:
  #     - ./public:/app/public
  #     - ./.env:/app/.env
  #   restart: unless-stopped

volumes:
  oraculo_volume:
